/**
 * The code in this class works with the following instances:
 * INPUT query file must have regular edges, and edges connecting intermediate nodes must be replaced with property with > 30000 id.
 * INPUT query log must be generated by replacing intermediate node edges by corresponding new edge ids.
 * The nodes in input query can be entities or type nodes.
 *
 * If we want this to work for queries where type nodes are considered, but without having new edges for intermediate node edges, THEN:
 * - we must keep the intermediateNodeList data structure empty. so we must comment out the line that populates it, lfm.loadIntermediateNodesList.
 * - also comment out the lfm.loadConcatedPropertiesList() function call. no need for this.
 * ****IMP***** THE ABOVE TWO CAN BE ACHIEVED by setting the "QuerySuggestion.ConcatenateIntermediatNodeEdges" flag to 0 in prop file. *******IMP********
 * - BUT, we will ALSO NEED the query log to be generated without replacing intermediate node edges with new edge IDs.
 *
 *  INPUT FILES:
 *  PropertyKeys.datagraphFile: data graph file
 *  PropertyKeys.edgeTypeFile: file containing the type of edge ends
 *  PropertyKeys.intermediateNodesFile: file containing list of intermediate nodes
 *  PropertyKeys.trainigDataWithIDFile: This is the USER LOG. Make sure this is the correct format user log you are using (no concat, or no newProp etc.)
 *
 */
package viiq.graphQuerySuggestionMain;

import it.unimi.dsi.lang.MutableString;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;

import ca.pfv.spmf.algorithms.associationrules.agrawal94_association_rules.AlgoAgrawalFaster94;
import ca.pfv.spmf.algorithms.associationrules.agrawal94_association_rules.AssocRules;
import ca.pfv.spmf.algorithms.frequentpatterns.fpgrowth.AlgoFPGrowth;
import ca.pfv.spmf.patterns.itemset_array_integers_with_count.Itemsets;

import org.apache.commons.configuration.ConfigurationException;
import org.apache.log4j.Logger;

import viiq.clientServer.server.LoadData;
import viiq.commons.EdgeEnds;
import viiq.commons.EdgeTypeInfo;
import viiq.commons.IntermediateNodeAndOtherEnd;
import viiq.commons.ObjNodeIntProperty;
import viiq.commons.CandidateEdgeScore;
import viiq.commons.GuiEdgeInfo;
import viiq.commons.GuiEdgeStringInfo;
import viiq.commons.QueryResult;
import viiq.commons.CandidateEdgeEnds;
import viiq.graphCompletionGuiMain.GenerateCandidatesNew;
import viiq.graphQuerySuggestionMain.DestNode;
import viiq.decisionForest.DecisionForestMain;
import viiq.otherClassifiers.baseLineRanker.BaseLineRanker;
import viiq.otherClassifiers.naiveBayesian.NaiveBayesianMain;
import viiq.otherClassifiers.randomEdgeSuggestion.RandomEdgeSuggestor;
import viiq.otherClassifiers.randomForest.RandomForestPredict;
import viiq.otherClassifiers.randomSubsets.RandomSubsetsMain;
import viiq.otherClassifiers.svd.RecommendationSystem;
import viiq.otherClassifiers.naiveBayesPython.NaiveBayes;
import viiq.otherClassifiers.randomForestPython.RandomForest;
import viiq.otherClassifiers.associationRule.AssociationRuleRecommender;
import viiq.otherClassifiers.cba.Cba;
import viiq.utils.BufferedRandomAccessFile;
import viiq.utils.PropertyKeys;
import viiq.backendHelper.SpringClientHelper;

public class GraphQueryAlgosComparisonMain {
	final Logger logger = Logger.getLogger(getClass());
	Config conf = null;

	/*
	 * Freebase specific data structures
	 */
	// set of nodes in the graph that are intermediate nodes.
	HashSet<Integer> intermediateNodesList = new HashSet<Integer>();
	// key = concatenation of two edges connecting an intermediate node, value = new edge ID corresponding to the key
	HashMap<String, Integer> concatenatedStringEdgesToNewEdgeIdMap = new HashMap<String, Integer>();
	// key: type, value: edges whose source type is the key. (generated using all the instances of "type" in the data graph)
	HashMap<Integer, HashSet<Integer>> sourceTypesToEdgesMap = new HashMap<Integer, HashSet<Integer>>();
	// key: type, value: edges whose object type is the key. (generated using all the instances of "type" in the data graph)
	HashMap<Integer, HashSet<Integer>> objectTypesToEdgesMap = new HashMap<Integer, HashSet<Integer>>();
	// key = edge, value = (source vertex Type, dest vertex Type)
	HashMap<Integer, EdgeTypeInfo> edgeType = new HashMap<Integer, EdgeTypeInfo>();
	BufferedRandomAccessFile sourceDataGraphFileHandler;
	BufferedRandomAccessFile objectDataGraphFileHandler;
	boolean concatenateIntermediatNodeEdges = true;

	/*
	 * DBpedia specific data structures
	 */
	// key: edge, value: candidate edges that can be incident on the source of the "key" edge
	HashMap<Integer, HashSet<Integer>> sourceEdgesToEdgesMapDbpedia = new HashMap<Integer, HashSet<Integer>>();
	// key: edge, value: candidate edges that can be incident on the object of the "key" edge
	HashMap<Integer, HashSet<Integer>> objectEdgesToEdgesMapDbpedia = new HashMap<Integer, HashSet<Integer>>();

	boolean loadDataGraph = false;
	HashMap<Integer, ArrayList<ObjNodeIntProperty>> srcDataGraph = null;
	HashMap<Integer, ArrayList<ObjNodeIntProperty>> objDataGraph = null;

	//0 = DecisionForest;	1 = Random Forests;	2 = Naive Bayesian model;	3 = Random edge suggestion;
	//4 = recommendation systems (SVD);	5 = RandomSubsets;	6 = CAR;
	enum ModelToUseForType {DF, RF, NBC, RAND, SVD, RandSubsets, CAR, ARM, BLR, HYB};
	ModelToUseForType model;
	boolean ignoreNegativeEdgesInHistory = false;
	boolean isFreebaseDataset = false;
	/*
	 * Model objects that can be used!
	 */
	//NaiveBayesianMain nbc;
	RandomSubsetsMain rsm;
	DecisionForestMain dfm;
	RandomEdgeSuggestor res;
	RecommendationSystem svd;
	BaseLineRanker blr;
	//RandomForestPredict rfp;
	NaiveBayes nb;
	RandomForest rf;
    Cba cba;
	AssociationRuleRecommender arm;

	double dfThreshold = 0.0; //assign value later

	//paramters for RDP
	int numOfRandomSubsets = 0;
	boolean isWeightedConf = false;
	int topkRules = 0;
	int countCondition = 0;
	int historyUpdate = 0;

	GraphQuerySuggestionHelper gqhelper = new GraphQuerySuggestionHelper();


	public static void main(String[] args) {

		GraphQueryAlgosComparisonMain gc = new GraphQueryAlgosComparisonMain();

		// AlgoApriori apriori = new AlgoApriori();
		// try {
		// 	Itemsets is = apriori.runAlgorithm(0.5, "/mounts/[server_name]/data/apriori_in", null);
		// 	AlgoAgrawalFaster94 ar = new AlgoAgrawalFaster94();
		// 	ar.runAlgorithm(is, "/mounts/[server_name]/data/arm_out", 4, 0.1);
		// } 
		// catch(IOException ioe) {
		// 		ioe.printStackTrace();
		// }

		if(args.length < 1) {
			System.out.println("Need an input properties file! Exiting program...");
			//btf.logger.error("Need an input properties file! Exiting program...");
		}
		else {
			try {
				gc.conf = new Config(args[0]);
			}
			catch(ConfigurationException ce) {
				System.out.println("Error in properties file configuration! Exiting program...");
				//	btf.logger.error("Error in properties file configuration! Exiting program...");
				ce.printStackTrace();
				return;
			}
			catch(IOException ioe) {
				System.out.println("IO exception while reading the properties file! Exiting program...");
				//	btf.logger.error("IO exception while reading the properties file! Exiting program...");
				ioe.printStackTrace();
				return;
			}
		}

		System.out.println("args length = "+args.length);

		if(args.length > 1) {
			gc.numOfRandomSubsets = Integer.parseInt(args[1]);
			gc.isWeightedConf     = Boolean.parseBoolean(args[2]);
			gc.topkRules          = Integer.parseInt(args[3]);
			gc.countCondition     = Integer.parseInt(args[4]);
			gc.historyUpdate      = Integer.parseInt(args[5]);
		}

		System.out.println("tau decay factor = " + Double.parseDouble(gc.conf.getProp(PropertyKeys.userLogSizeThresholdDecayFactorMultiplier)) );
		System.out.println("numerator power = "  + Double.parseDouble(gc.conf.getProp(PropertyKeys.numeratorPower)) );	

		if(Integer.parseInt(gc.conf.getProp(PropertyKeys.loadDataGraphFlag)) == 1)
			gc.loadDataGraph = true;
		/*
		 * dataGraphInUse, tells us which data graph is used:
		 * 0 = Freebase
		 * 1 = DBpedia
		 * 2 = YAGO
		 */
		int dataGraphInUse = Integer.parseInt(gc.conf.getProp(PropertyKeys.datasetFlag));
		if(dataGraphInUse == 0) {
			// This is freebase
			// load all necessary files.
			gc.isFreebaseDataset = true;
			gc.initializaFreebaseDataStructures();
		} else if(dataGraphInUse == 1) {
			// This is DBpedia
			gc.isFreebaseDataset = false;
			gc.initializeDBpediaDataStructures();
		}

		System.out.println("Done loading input data");
		File inputQueryFiles = new File(gc.conf.getInputFilePath(PropertyKeys.testPartialAndTargetQueryFiles));

		// create buffered random access file handlers

		try {
			if(!gc.loadDataGraph && gc.isFreebaseDataset) {
				int numOfTotalEdges = Integer.parseInt(gc.conf.getProp(PropertyKeys.datagraphNumberOfLines));
				gc.sourceDataGraphFileHandler = new BufferedRandomAccessFile(gc.conf.getInputFilePath(PropertyKeys.datagraphSourceAlignedFile), "r", numOfTotalEdges, gc.conf);
				gc.objectDataGraphFileHandler = new BufferedRandomAccessFile(gc.conf.getInputFilePath(PropertyKeys.datagraphObjectAlignedFile), "r", numOfTotalEdges, gc.conf);
			}

			gc.queryCompletor(inputQueryFiles);

			if(!gc.loadDataGraph && gc.isFreebaseDataset) {
				gc.sourceDataGraphFileHandler.close();
				gc.objectDataGraphFileHandler.close();
			}
		} catch(IOException ioe) {
			ioe.printStackTrace();
		}
		System.out.println("Done with completing all queries!!!");
	}

	private ModelToUseForType getModelToUseForType(int mod){
		//0 = DecisionForest;	1 = Random Forests;	2 = Naive Bayesian model;	3 = Random edge suggestion;
		//4 = recommendation systems (SVD);	5 = RandomSubsets;	6 = CAR;
		//enum ModelToUseForType {DF, RF, NBC, RAND, SVD, RandSubsets, CAR};
		ModelToUseForType model;
		if(mod == 0)
			model = ModelToUseForType.DF;
		else if(mod == 1)
			model = ModelToUseForType.RF;
		else if(mod == 2)
			model = ModelToUseForType.NBC;
		else if(mod == 3)
			model = ModelToUseForType.RAND;
		else if(mod == 4)
			model = ModelToUseForType.SVD;
		else if(mod == 5)
			model = ModelToUseForType.RandSubsets;
		else if(mod == 6)
			model = ModelToUseForType.CAR;
		else if(mod == 7)
			model = ModelToUseForType.ARM;
		else if(mod == 8)
			model = ModelToUseForType.BLR;	
		else if(mod == 9)
			model = ModelToUseForType.HYB;		
		else {
			System.out.println("model = "+mod);
			// mark NBC as the default model if the properties file has an unrecognized model.
			System.out.println("The properties file has an unrecognized model. Using Bayesian as default!");
			//			logger.warn("The properties file has an unrecognized model. Using Bayesian as default!");
			model = ModelToUseForType.DF;
		}
		return model;
	}

	private void queryCompletor(File inputFilesFolder) {

		dfThreshold = Double.parseDouble(conf.getProp(PropertyKeys.decayFactorThreshold));
		System.out.println("Decay factor threshold = "+dfThreshold);

		model = getModelToUseForType(Integer.parseInt(conf.getProp(PropertyKeys.modelToUse)));

		if(model == ModelToUseForType.HYB) { // learn both rdp and baseline
			dfm = new DecisionForestMain(conf);
			blr = new BaseLineRanker(conf);

			System.out.println("Learning model DECISON FOREST");
			dfm.learnModel(numOfRandomSubsets, isWeightedConf, topkRules, countCondition, historyUpdate);
			System.out.println("Learning model BLR");
			blr.learnModel();
		} else {
			learnModel();
		}


		if(Integer.parseInt(conf.getProp(PropertyKeys.ignoreNegativeEdges)) == 1) {
			ignoreNegativeEdgesInHistory = true;
		}
		String outQueryFilesFolder = conf.getOutputFilePath(PropertyKeys.edgeSuggestionOutputFolder);
		File[] listOfInputQueryFiles = inputFilesFolder.listFiles();
		for(int i=0; i<listOfInputQueryFiles.length; i++) {
			// Each file must contain the partial query graph and the target query graph.
			if(listOfInputQueryFiles[i].isFile()) {
				String outFileName = listOfInputQueryFiles[i].getName();
				System.out.println("\nStarting query completion for file " + outFileName);
				String inputQueryFilePath = listOfInputQueryFiles[i].getAbsolutePath();
				/*
				 * This graph representation stores the input graph and the target graph (which is to be used in prototype). In the actual
				 * system, we are unaware of the targetGraph, only the user knows it.
				 * key = edge ID
				 * value = edge ends (src, obj)
				 *
				 * NOTE: If, a key has "null" as its value, it means the key represents an Vertex ID.
				 * if value = null,
				 * key = vertex ID.
				 *
				 * The null case is not supposed to occur in the target graph. The target graph MUST be connected.
				 */
				HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph = new HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>>();
				HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph = new HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>>();
				ArrayList<HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>>> allPartialGraphs = gqhelper.readInputQueryFileAddGraphNode(inputQueryFilePath, partialGraph, targetGraph);
				ArrayList<HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>>> allTargetGraphs = makeDeepCopies(targetGraph, allPartialGraphs.size());
				
				System.out.println("\nTarget graph:");
				gqhelper.printConstructedGraphLabel(allTargetGraphs.get(0));
				
				for(int j=0; j<allPartialGraphs.size(); j++) {
					String outputTargetQueryFilePath = outQueryFilesFolder + outFileName + "-" + j;
					
					long startTime = java.lang.System.currentTimeMillis();
					makeEdgeSuggestions(allPartialGraphs.get(j), allTargetGraphs.get(j));
					long endTime = java.lang.System.currentTimeMillis();
					System.out.println("Time taken for completing this graph (in ms) = " + (endTime - startTime));
					//gqhelper.printConstructedGraph(allPartialGraphs.get(j), outputTargetQueryFilePath);
					System.out.println("COMPLETED query completion for file " + outFileName);
				}
			}
			if(model == ModelToUseForType.DF) { //clear cached results to free up memory
				dfm.clearCache();
			}
		}
	}

	private void makeEdgeSuggestions(HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
		HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph) {

		
		LoadData ldlm = new LoadData();
		
		SpringClientHelper scp = new SpringClientHelper(conf);
		HashMap<String, Double> decayFactor = new HashMap<String, Double>();
		double _dfm = Double.parseDouble(conf.getProp(PropertyKeys.decayFactorMultiplier));
		double logdfm = Double.parseDouble(conf.getProp(PropertyKeys.userLogSizeThresholdDecayFactorMultiplier));

		model = getModelToUseForType(Integer.parseInt(conf.getProp(PropertyKeys.modelToUse)));
		
		HashSet<Integer> history = new HashSet<Integer>();

		initializeTargetGraph(partialGraph, targetGraph);

		int numOfSuggestions = 0;
		boolean initGraph = true;
		boolean newGraph = true;
		//boolean newAlgo = true;

		ArrayList<GuiEdgeInfo> pg = null;

		QueryResult qr = null;
		
		HashMap<Integer, ArrayList<CandidateEdgeEnds>> edgeToTripletMap = new HashMap<Integer, ArrayList<CandidateEdgeEnds>>();
		HashSet<Integer> candidateEdges = null;

		ArrayList<CandidateEdgeScore> edgeScoresRDP = null;
		ArrayList<CandidateEdgeScore> edgeScoresBaseline = null; 

		boolean isSuccess  = true;

		double logdf = 1.0; //decay factor to dynamically reduce the log size threshold so that the constraint become more flexible at each iteration
		
		while(!targetGraph.isEmpty()) {
			String systemName = (model == ModelToUseForType.HYB) 
								? (((numOfSuggestions & 1) == 0 || !isSuccess)? "baseline"  : "orion") //choose baseline if previous iteration was rdp, or rdp has already failed 
								: ((model == ModelToUseForType.BLR) ? "baseline"  : "orion") ;

			
			double edgeScoreThreshold =  Double.parseDouble(conf.getProp(
											(systemName.equals("baseline")) 
											? PropertyKeys.edgeScoreThresholdBLR : PropertyKeys.edgeScoreThresholdRDP
										));

			GenerateCandidatesNew gc = new GenerateCandidatesNew(conf);
			
			if(newGraph) {

				pg = partialGraphToGuiGraph(partialGraph);
				qr = scp.evaluateQueryGraph(pg);
				
				// //print full pg
				// for(GuiEdgeInfo e : pg) {
				// 	System.out.println(e.source+" : "+e.graphSource+" : "+qr.values.get(e.graphSource).size()+" : "+e.sourceTypeValues+" : "+e.sourceEntity+" : "+e.edge+" : "+e.object+" : "+e.graphObject+" : "+qr.values.get(e.graphObject).size()+" : "+e.objectTypeValues+" : "+e.objectEntity);
				// }	

				if(initGraph == true) {
					System.out.println("\n---------------------------------------------------------------\nInitial query graph:");
					initGraph = false;
				} else {
					System.out.println("\n---------------------------------------------------------------\nCurrent query graph:");
				}

				gqhelper.printConstructedGraphLabel(partialGraph);


				System.out.println("\nGraphNode to node-label mapping:");
				for(int graphNode : qr.values.keySet()) {
					for(GuiEdgeInfo gei : pg) {
						//System.out.println(gei.source+", "+gei.object+", "+gei.graphSource+", "+gei.graphObject+", "+gei.edge);
						if(gei.graphSource == graphNode) {
							System.out.println("	"+graphNode+" -> "+ldlm.getNodeLabelIndex().get(gei.source));
							break;
						} else if(gei.graphObject == graphNode) {
							System.out.println("	"+graphNode+" -> "+ldlm.getNodeLabelIndex().get(gei.object));
							break;
						}
					}			
				}
				
			// }

			// if(newAlgo || newGraph) {
				gc.getPartialGraph(1, pg, history, null, null, conf);
				candidateEdges = gc.getCandidateEdges(pg, 1, null, edgeToTripletMap, 0, 0, qr.values, systemName, conf);

				//edgeScores = rankCandidateEdges(history, qr, candidateEdges, logdf, systemName);

				edgeScoresRDP = null;
				edgeScoresBaseline = null;
				newGraph = false;

			} 

			System.out.println("\nsystem name = "+systemName.toUpperCase());
	
			
			//newAlgo = (model == ModelToUseForType.HYB) ? true : false; // switch models alternatively for hybrid method
			
			ArrayList<GuiEdgeStringInfo> suggestions = null;
			
			if(systemName.equals("orion")) {
				if(edgeScoresRDP == null || logdfm < 1.0) {
					System.out.println("Edge score recalculating.");
					edgeScoresRDP = dfm.rankCandidateEdges(history, candidateEdges, logdf);
				} 
				suggestions = gc.getCandidateTriples(pg, 1, 1, edgeScoresRDP, decayFactor, new HashSet<String>(), edgeToTripletMap, systemName, conf).rankedUniqueEdges;
			}
			else {
				if(edgeScoresBaseline == null) {
					System.out.println("Edge score recalculating.");
					edgeScoresBaseline = blr.rankCandidateEdges(candidateEdges, qr);
				} 
				suggestions = gc.getCandidateTriples(pg, 1, 1, edgeScoresBaseline, decayFactor, new HashSet<String>(), edgeToTripletMap, systemName, conf).rankedUniqueEdges;
			}




			// if(systemName.equals("orion")) {
			// 	boolean foundGoodSuggestion = false;
			// 	for(GuiEdgeStringInfo gei :  suggestions) {
			// 		//look for edges which are non zero score and also has decay factor above threshold
			// 		//we treat zero score edges as special, since these edges may be required in case the query log is not rich enough
			// 		String triple = gei.graphSource+","+gei.edge.split("\\|")[0]+","+gei.graphObject;
			// 		if((gei.score > 0.0 && decayFactor.get(triple) > dfThreshold) || gei.score == 0) {
			// 			foundGoodSuggestion = true;
			// 		}
			// 	}
			// 	if(!foundGoodSuggestion) {
			// 		System.out.println("all non-zero score's decayfactor under threshold... terminating.");
			// 		break;
			// 	}
			// }


			if(suggestions.size()==0) {
				System.out.println("no bestEdge found ... terminating.");
				isSuccess = false;
				break;
			}

			GuiEdgeStringInfo topSuggestion = suggestions.get(0);

			int suggestedEdge = Integer.parseInt(topSuggestion.edge.split("\\|")[0]);

			String bestTriple = topSuggestion.graphSource+ "," + suggestedEdge + "," + topSuggestion.graphObject;

			System.out.println("tau decay factor = "+ logdf);

			System.out.printf("Best triple = <"+topSuggestion.graphSource+","+ldlm.getEdgeLabelIndex().get(suggestedEdge)+","+topSuggestion.graphObject+">, bestScore (without df)= %.5f, edge decay factor= %.3f, bestScore (with df)= %.5f\n", topSuggestion.score/decayFactor.get(bestTriple), decayFactor.get(bestTriple), topSuggestion.score);

			// for(CandidateEdgeScore ces : edgeScoresBaseline) {
			// 	String triple = ces.isForwardEdge ? ces.node+","+ces.edge+",-1" :  "-1,"+ces.edge+","+ces.node;
			// 	if(triple.equals(bestTriple)) {
			// 		System.out.println("Baseline score for the above best triple = "+ces.score);
			// 		break;
			// 	}
			// }

			if(topSuggestion.score <= edgeScoreThreshold
				//||  !systemName.equals("hybrid") //for hybrid method
				//&& !systemName.equals("orion") //for rdp simulated experiments show all zero score edges
			) {
				// if(model == ModelToUseForType.DF) {
				// 	System.out.println("bestEdge score below threshold ... switching from RDP to Baseline.");
				// 	model = ModelToUseForType.BLR;	
				// 	newAlgo = true;
				// 	continue;
				// } else {
					System.out.println("bestEdge score below threshold ... terminating.");
					isSuccess = false;
					if(model == ModelToUseForType.HYB) continue;
					else break;
				// }
			} 


			ArrayList<EdgeEnds> positiveEdges = null;

			if((positiveEdges = isPositiveSuggestedEdge(bestTriple, partialGraph, targetGraph)) != null) {
				System.out.println("***Recommendation accepted!***");
				gqhelper.addEdgesToPartialGraph(suggestedEdge, positiveEdges, partialGraph);
				gqhelper.removeEdgesFromTargetGraph(suggestedEdge, positiveEdges, targetGraph);
				
				edgeToTripletMap.clear();
				newGraph = true;
			} else {
				System.out.println("***Recommendation rejected.***");
				decayFactor.put(bestTriple, decayFactor.get(bestTriple)*_dfm);
			}
			numOfSuggestions++;
			System.out.println("Iteration #"+numOfSuggestions+" complete.");

			if(systemName.equals("orion")) logdf *= logdfm; //decay factor on \tau will only be applied if the algorithm is rdp
		}

		if(!isSuccess && model != ModelToUseForType.HYB) numOfSuggestions = -1; // hybrid is confirm to reach to graph, even though isSuccess has -1 value due to RDP failure at earlier iteration
		System.out.println("NUMBER of SUGGESTIONS MADE : " + numOfSuggestions);	
	}

	// private void makeEdgeSuggestionsOld(HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
	// 		HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph) {
	// 	// contains the candidate graph, which is changed after every edge suggestion. We will delete an edge if it is accepted (but added
	// 	// into the partial graph). Edges are only deleted if it is rejected.
	// 	// Note that, all edges of the same label are deleted from the candidate graph (assuming a yes or no is said to all edges
	// 	// that are incident on the partial graph, which is the current instance of the candidate graph.
	// 	// key = edge, value = <src, obj, tupleID>
	// 	HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph = new HashMap<Integer, ArrayList<EdgeEnds>>();
	// 	HashSet<Integer> candidateEdges = new HashSet<Integer>();
	// 	HashSet<Integer> history = new HashSet<Integer>();
	// 	// this contains the tuple IDs seen so far. This includes both the ones present in the partial graph and those rejected too.
	// 	HashSet<MutableString> seenTuples = new HashSet<MutableString>();

	// 	initializeHistorySeenTuplesListAndTargetGraph(history, seenTuples, partialGraph, targetGraph);
	// 	if(isFreebaseDataset) {
	// 		initializeCandidateEdges(candidateGraph, partialGraph, candidateEdges, seenTuples);
	// 	} else {
	// 		// dbpedia
	// 		initializeCandidateEdgesDbpedia(partialGraph, candidateEdges);
	// 	}

	// 	LoadData ldlm = new LoadData();

	// 	int numOfSuggestionsThreshold = Integer.parseInt(conf.getProp(PropertyKeys.NumberOfSuggestionsThreshold));
	// 	int numOfSuggestions = 0;
	// 	boolean initGraph = true;
	// 	HashSet<Integer> correctSuggestions = new HashSet<Integer>();
	// 	//long suggestionTime = 0;
		
	// 	HashMap<String, Double> decayFactor = new HashMap<String, Double>();
	// 	double dfm = Double.parseDouble(conf.getProp(PropertyKeys.decayFactorMultiplier));

	// 	SpringClientHelper scp = new SpringClientHelper(conf);
	// 	QueryResult qr = new QueryResult();
	// 	if(model == ModelToUseForType.BLR) {
	// 		ArrayList<GuiEdgeInfo> guiGraph = partialGraphToGuiGraph(partialGraph);
	// 		// for(GuiEdgeInfo e : guiGraph) {
	// 		// 	System.out.println(e.source+", "+e.graphSource+", "+e.object+", "+e.graphObject+", "+e.edge+", "+e.sourceTypeValues+", "+e.objectTypeValues+", "+e.sourceEntity+", "+e.objectEntity);
	// 		// }
	// 		qr = scp.evaluateQueryGraph(guiGraph);
	// 	}

	// 	boolean newGraph = true;

	// 	ArrayList<CandidateEdgeScore> edgeScores = new ArrayList<CandidateEdgeScore>();

	// 	//while(numOfSuggestions < numOfSuggestionsThreshold && !targetGraph.isEmpty())
	// 	while(!targetGraph.isEmpty()) {
	// 		//long startTime = java.lang.System.currentTimeMillis();
	// 		//System.out.println("candidateEdges size = "+candidateEdges.size());

	// 		ArrayList<GuiEdgeInfo> guiGraph = partialGraphToGuiGraph(partialGraph);

	// 		if(newGraph) {

	// 			// if(candidateEdges.contains(47185839)) {
	// 			// 	System.out.println("contains 47185839");
	// 			// }
				

	// 			// String initialGraph = "";
	// 			// for(int src : partialGraph.keySet()) {
	// 			// 	for(int prop : partialGraph.get(src).keySet()) {
	// 			// 		initialGraph = ldlm.getEdgeLabelIndex().get(prop) + " , " + ldlm.getNodeLabelIndex().get(src) + " : " + ldlm.getNodeLabelIndex().get(partialGraph.get(src).get(prop).get(0).getDest());
	// 			// 	}
	// 			// }

	// 			if(initGraph == true) {
	// 				System.out.println("\n---------------------------------------------------------------\nInitial query graph:");
	// 				initGraph = false;
	// 			} else {
	// 				System.out.println("\n---------------------------------------------------------------\nCurrent query graph:");
	// 			}
					

	// 			//System.out.println(initialGraph);
	// 			gqhelper.printConstructedGraphLabel(partialGraph);
	// 			if(model == ModelToUseForType.BLR) {
	// 				System.out.println("\nGraphNode to node-label mapping:");
	// 				for(int graphNode : qr.values.keySet()) {
	// 					for(GuiEdgeInfo gei : guiGraph) {
	// 						//System.out.println(gei.source+", "+gei.object+", "+gei.graphSource+", "+gei.graphObject+", "+gei.edge);
	// 						if(gei.graphSource == graphNode) {
	// 							System.out.println("	"+graphNode+" -> "+ldlm.getNodeLabelIndex().get(gei.source));
	// 							break;
	// 						} else if(gei.graphObject == graphNode) {
	// 							System.out.println("	"+graphNode+" -> "+ldlm.getNodeLabelIndex().get(gei.object));
	// 							break;
	// 						}
	// 					}			
	// 				}
	// 			}
	// 		}


	// 		if(newGraph || model != ModelToUseForType.BLR) {
	// 			edgeScores = rankCandidateEdges(history, qr, candidateEdges);
	// 		}

	// 		// double maxDf = -1.0;
	// 		// for(CandidateEdgeScore ces : edgeScores) {
	// 		// 	if(!decayFactor.containsKey(ces.edge)) {
	// 		// 		decayFactor.put(ces.edge, 1.0);
	// 		// 	}
	// 		// 	maxDf = Math.max(maxDf, decayFactor.get(ces.edge));
	// 		// }
			
	// 		// //System.out.println("edgeScores size = "+edgeScores.size()+", candidate score = "+candidateEdges.size());
	// 		// if(maxDf < Double.parseDouble(conf.getProp(PropertyKeys.decayFactorThreshold))) {
	// 		// 	System.out.println("All candidate edge decay factors less than threshold ... terminating.");
	// 		// 	break;
	// 		// }

	// 		double bestScore = -1.0;
	// 		double bestScoreWithoutDf = -1.0;
	// 		String bestTriple = "";
			
	// 		for(CandidateEdgeScore e : edgeScores) {
	// 			String triple = e.isForwardEdge ? e.node+","+e.edge+",-1" : "-1,"+e.edge+","+e.node;
				
	// 			double df = (decayFactor.getOrDefault(triple, 1.0) > Double.parseDouble(conf.getProp(PropertyKeys.decayFactorThreshold)) ) ? decayFactor.getOrDefault(triple, 1.0) : 0.0;
	// 			if(e.score * df > bestScore && ldlm.getEdgeLabelIndex().containsKey(e.edge) ) {
	// 				bestScore = e.score * df;
	// 				bestScoreWithoutDf = e.score;
	// 				bestTriple = triple;
	// 			}

	// 			//find cancdidate edges between existing nodes
	// 			for(CandidateEdgeScore re : edgeScores) {
	// 				if(e.edge==re.edge && e.node != re.node && e.score > 0 && re.score > 0) {
	// 					int src = -1, obj = -1;
	// 					if(e.isForwardEdge && !re.isForwardEdge) {
	// 						src = e.node;
	// 						obj = re.node;	
	// 					} else if(!e.isForwardEdge && re.isForwardEdge) {
	// 						src = re.node;
	// 						obj = e.node;	
	// 					}
	// 					String tripleBetweenNodes = src+","+e.edge+","+obj;
	// 					boolean edgeAlreadyExists = false;
	// 					for(GuiEdgeInfo gei : guiGraph) {
	// 						if(gei.graphSource == src && gei.edge == e.edge && gei.graphObject == obj) {
	// 							edgeAlreadyExists = true; //do not recommend the same edge between existing nodes if those are already connected by that edge
	// 							break;
	// 						}
	// 					}
						
	// 					if(!edgeAlreadyExists && src != -1 && obj !=-1) {
	// 						df = (decayFactor.getOrDefault(tripleBetweenNodes, 1.0) > Double.parseDouble(conf.getProp(PropertyKeys.decayFactorThreshold)) ) ? decayFactor.getOrDefault(tripleBetweenNodes, 1.0) : 0.0;
	// 						if(Math.max(e.score, re.score) * df > bestScore && ldlm.getEdgeLabelIndex().containsKey(e.edge) ) {
	// 							bestScore = Math.max(e.score, re.score) * df;
	// 							bestScoreWithoutDf = Math.max(e.score, re.score);
	// 							bestTriple = tripleBetweenNodes;
	// 						}
	// 					}
	// 				}
	// 			}
	// 		}

	// 		int suggestedEdge = Integer.parseInt(bestTriple.split(",")[1]);

	// 		System.out.printf("Best triple = <"+bestTriple.split(",")[0]+","+ldlm.getEdgeLabelIndex().get(suggestedEdge)+","+bestTriple.split(",")[2]+">, bestScore (without df)= %.3f, decay factor= %.3f, bestScore (with df)= %.3f\n", bestScoreWithoutDf, decayFactor.getOrDefault(bestTriple, 1.0), bestScore);

	// 		if(bestScore < Double.parseDouble(conf.getProp(PropertyKeys.edgeScoreThresholdBLR))) {
	// 			System.out.println("bestEdge score below threshold ... terminating.");
	// 			break;
	// 		} 

			
	// 		//suggestionTime += (java.lang.System.currentTimeMillis() - startTime);
	// 		// if(suggestedEdge <= 0) {
	// 		// 	//System.out.println("Suggested edge was a 0.. breaking out..");
	// 		// 	//				logger.warn("There was no edge in candidateEdges??");
	// 		// 	suggestedEdge = RandomEdgeSuggestor.getRandomEdge(candidateEdges);
	// 		// 	//System.out.println(suggestedEdge);
	// 		// 	//break;
	// 		// }
	// 		//candidateEdges.remove(suggestedEdge);
			
	// 		int prop = suggestedEdge;
	// 		//		boolean previouslyAcceptedNowRejectedEdge = false;
	// 		ArrayList<EdgeEnds> positiveEdges = null;

	// 		if((positiveEdges = isPositiveSuggestedEdge(bestTriple, partialGraph, targetGraph)) != null) {
	// 			//System.out.println("positiveEdges edges found!");
	// 			if(partialGraph.isEmpty()) {
	// 				// The input query graph was empty and this was the first edge to be accepted. Start over with the candidates.
	// 				// if there is even a single edge in the partial query graph, it means candidate edges list will be appended to.
	// 				candidateEdges.clear();
	// 			}

	// 			// System.out.println("target graph:");
	// 			// for(int src : targetGraph.keySet()) {
	// 			// 	for(int p : targetGraph.get(src).keySet()) {
	// 			// 		for(DestNode dn : targetGraph.get(src).get(p)) {
	// 			// 			System.out.println(src+","+p+","+dn.getDest()+","+dn.getDestGraphNodeID());
	// 			// 		}
						
	// 			// 	}
	// 			// }

	// 			gqhelper.addEdgesToPartialGraph(suggestedEdge, positiveEdges, partialGraph);

	// 			// System.out.println("partial graph:");
	// 			// for(int src : partialGraph.keySet()) {
	// 			// 	for(int p : partialGraph.get(src).keySet()) {
	// 			// 		for(DestNode dn : partialGraph.get(src).get(p)) {
	// 			// 			System.out.println(src+","+p+","+dn.getDest()+","+dn.getDestGraphNodeID());
	// 			// 		}
						
	// 			// 	}
	// 			// }
	// 			gqhelper.removeEdgesFromTargetGraph(suggestedEdge, positiveEdges, targetGraph);
	// 			correctSuggestions.add(prop);
	// 		} else {
	// 			//	if(correctSuggestions.contains(prop))
	// 			//		previouslyAcceptedNowRejectedEdge = true;
	// 			prop = prop*(-1);
	// 		}
	// 		if(isFreebaseDataset) {
	// 			// add all the tuple IDs in candidate graph corresponding to a given edge label to seenTupleIDs.
	// 			// remove all the instances of the suggested edge from candidate graph.
	// 			/**
	// 			 * This code does not seem to be very relevant anymore. But not changing it since its already in a working condition for
	// 			 * freebase! I know, that's a bad bad reason. But I don't have the time to take the risk of changing stuff now!
	// 			 */
	// 			updateSeenTupleIDsListAndCandidateGraph(seenTuples, suggestedEdge, candidateGraph);
	// 		}

	// 		if(prop > 0) {
	// 			// this means that the newly suggested edge was accepted. So we may get to add new candidate edges.
	// 			System.out.println("***Recommendation accepted!***");
	// 			if(isFreebaseDataset) {
	// 				findNewCandidateEdges(suggestedEdge, candidateGraph, positiveEdges, candidateEdges, seenTuples);
	// 			} else {
	// 				findNewCandidateEdgesDbpedia(suggestedEdge, positiveEdges, candidateEdges);
	// 			}
	// 			if(model == ModelToUseForType.BLR) { //CHANGE IT LATER
	// 				qr = scp.evaluateQueryGraph(partialGraphToGuiGraph(partialGraph));
					
	// 			}
	// 			newGraph = true;
	// 		} else {
	// 			System.out.println("***Recommendation rejected.***");
	// 			decayFactor.put(bestTriple, decayFactor.getOrDefault(bestTriple, 1.0)*dfm);
	// 			newGraph = false;
	// 		}

	// 		// add the suggested edge to history only if its not already present in the history,
	// 		// AND, if this edge was rejected, add it iff it was not accepted in a previous iteration.
	// 		boolean addEdgeToHistory = true;
	// 		if(ignoreNegativeEdgesInHistory) {
	// 			if(prop < 0)
	// 				addEdgeToHistory = false;
	// 		} else {
	// 			int posProp = prop;
	// 			if(prop < 0)
	// 				posProp = prop*(-1);
	// 			for(int e : history) {
	// 				// do not add edge, if the accepted version of THIS rejected edge was already in the hisory.
	// 				// do not add edge, if the same edge was rejected earlier too.
	// 				if(e == posProp || e == prop) {
	// 					addEdgeToHistory = false;
	// 					break;
	// 				}
	// 			}
	// 		}
	// 		if(addEdgeToHistory) {
	// 			history.add(prop);
	// 			//adding types to query session
	// 			if(positiveEdges != null) {
	// 				for(EdgeEnds e : positiveEdges) {
	// 					history.add(e.source);
	// 					history.add(e.object);
	// 				}
	// 			}
	// 		}
				
			
	// 		numOfSuggestions++;
	// 		System.out.println("Iteration #"+numOfSuggestions+" complete.");
			
	// 	}
	// 	//System.out.println("Time for generating suggestions : " + suggestionTime/1000.0+ " seconds.");
	// 	System.out.println("NUMBER of SUGGESTIONS MADE : " + numOfSuggestions);
	// 	// if(numOfSuggestions == numOfSuggestionsThreshold) {
	// 	// 	System.out.println("100 suggestions up!!!!!!!!!");
	// 	// }
	// }

	private void initializeHistorySeenTuplesListAndTargetGraph(HashSet<Integer> history, HashSet<MutableString> seenTuples,
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph) {
		// look at the initial partial graph and add edges present in it to seenTuples.
		// also add it to the history.
		// if there are no edges in the partial query graph (only nodes), then seentuples and history are empty.
		// the edges found in the partial graph must be removed from the target graph.
		Iterator<Integer> iter = partialGraph.keySet().iterator();
		HashSet<Integer> seenProps = new HashSet<Integer>();
		while(iter.hasNext()) {
			int src = iter.next();
			HashMap<Integer, ArrayList<DestNode>> propDest = partialGraph.get(src);
			if(propDest != null) {
				Iterator<Integer> iter1 = propDest.keySet().iterator();
				while(iter1.hasNext()) {
					int prop = iter1.next();
					if(!seenProps.contains(prop)) {
						history.add(prop);
						seenProps.add(prop);

						//adding types to query session
						history.add(src);
						seenProps.add(prop);
					}
					ArrayList<DestNode> dns = propDest.get(prop);
					for(DestNode dn : dns) {
						if(dn.isForwardEdge()) {
							seenTuples.add(getTupleID(src, prop, dn.getDest()));
							removeInitialEdgesFromTargetGraph(src, prop, dn.getDest(), targetGraph);
						}
						//adding types to query session
						history.add(dn.getDest());
						seenProps.add(dn.getDest());
					}
				}
			}
		}
	}

	private void initializeTargetGraph(
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph) {
		// the edges found in the partial graph must be removed from the target graph.
		Iterator<Integer> iter = partialGraph.keySet().iterator();
		while(iter.hasNext()) {
			int src = iter.next();
			HashMap<Integer, ArrayList<DestNode>> propDest = partialGraph.get(src);
			if(propDest != null) {
				Iterator<Integer> iter1 = propDest.keySet().iterator();
				while(iter1.hasNext()) {
					int prop = iter1.next();
					ArrayList<DestNode> dns = propDest.get(prop);
					for(DestNode dn : dns) {
						if(dn.isForwardEdge()) {
							removeInitialEdgesFromTargetGraph(src, prop, dn.getDest(), targetGraph);
						}
						//adding types to query session
					}
				}
			}
		}
	}

	private void findNewCandidateEdgesDbpedia(int prop, ArrayList<EdgeEnds> positiveEdges, HashSet<Integer> candidateEdges) {
		HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> pseudoPartialGraph = new HashMap<Integer, HashMap<Integer,ArrayList<DestNode>>>();
		gqhelper.addEdgesToPartialGraph(prop, positiveEdges, pseudoPartialGraph);
		initializeCandidateEdgesDbpedia(pseudoPartialGraph, candidateEdges);
	}

	private void initializeCandidateEdgesDbpedia(HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
			HashSet<Integer> candidateEdges) {
		/**
		 * This is a much more mellowed version of initializeCandidateEdges() since this is called only for DBpedia. I know my query graphs
		 * will not have entity nodes, and there are no intermediate nodes in DBpedia.
		 */
		/**
		 * The candidate edges based on a given edge contains some edge IDs with a negative number too. It is not very useful here, but if
		 * we ever use that for an online demo, we will need it since the -ve sign represents the direction of the edge. If we are looking
		 * at the source Edge file, then a -ve edge basically means, that edge is incident on the SOURCE node of the current edge we are
		 * looking at, but the the SOURCE node is the destination of that -ve edge. If its a positive edge, then the SOURCE node is also
		 * the source node for the new edge +ve in the candidate list.
		 */
		Iterator<Integer> iter = partialGraph.keySet().iterator();
		while(iter.hasNext()) {
			int node = iter.next();
			HashMap<Integer, ArrayList<DestNode>> neighs = partialGraph.get(node);
			Iterator<Integer> propIter = neighs.keySet().iterator();
			while(propIter.hasNext()) {
				int edge = propIter.next();
				if(sourceEdgesToEdgesMapDbpedia.containsKey(edge)) {
					HashSet<Integer> cands = sourceEdgesToEdgesMapDbpedia.get(edge);
					addToCandidatesListDbpedia(candidateEdges, cands);
				}
				if(objectEdgesToEdgesMapDbpedia.containsKey(edge)) {
					HashSet<Integer> cands = objectEdgesToEdgesMapDbpedia.get(edge);
					addToCandidatesListDbpedia(candidateEdges, cands);
				}
			}
		}
	}

	private void addToCandidatesListDbpedia(HashSet<Integer> candidateEdges, HashSet<Integer> cands) {
		Iterator<Integer> it = cands.iterator();
		while(it.hasNext()) {
			int candedge = it.next();
			if(candedge < 0)
				candidateEdges.add(candedge*(-1));
			else
				candidateEdges.add(candedge);
		}
	}

	private void findNewCandidateEdges(int prop, HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph,
			ArrayList<EdgeEnds> positiveEdges, HashSet<Integer> candidateEdges, HashSet<MutableString> seenTuples) {
		// in this method, add all new edges that come into candidateGraph due to the addition of positiveEdges into the
		// partial graph.
		// fist construct a pseudo-partial graph that consists of only edges present in partial graph,
		// and input this to initializeCandidateEdges method. that should find all new edges that come in candidateGraph due to the
		// new edges found in positiveEdges.
		HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> pseudoPartialGraph = new HashMap<Integer, HashMap<Integer,ArrayList<DestNode>>>();
		gqhelper.addEdgesToPartialGraph(prop, positiveEdges, pseudoPartialGraph);
		initializeCandidateEdges(candidateGraph, pseudoPartialGraph, candidateEdges, seenTuples);
	}

	private void addTypeEdge(HashSet<MutableString> addedTuples, HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph,
			HashSet<Integer> candidateEdges, int edge, int srcType, int objType) {
		MutableString tid = getTupleID(srcType, edge, objType);
		if(!addedTuples.contains(tid)) {
			EdgeEnds ee = new EdgeEnds();
			ee.source = srcType;
			ee.object = objType;
			addToCandidateGraph(candidateGraph, candidateEdges, addedTuples, edge, ee, tid);
		}
	}

	private void initializeCandidateEdges(HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph,
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
			HashSet<Integer> candidateEdges, HashSet<MutableString> seenTuples) {
		// for every node present in the initial partial graph, add the neighboring edges (not in the partial graph) to the candidate
		// graph. also add these edges to candidate edges list.
		HashSet<MutableString> addedTuples = new HashSet<MutableString>();
		if(partialGraph.isEmpty()) {
			// The input could be empty. All edges are candidate edges.
			if(candidateEdges.isEmpty()) {
				// this is the first time we are even coming to this method. just populate all edges into candidate edges, until we
				// hit at least one positive edge suggestion.
				// if this is not the first time we have come in here, then the set of candidate edges is already populated.
				// the just suggested edge was also removed.
				Iterator<Integer> iter = edgeType.keySet().iterator();
				while(iter.hasNext()) {
					candidateEdges.add(iter.next());
				}
			}
		} else {
			Iterator<Integer> iter = partialGraph.keySet().iterator();
			while(iter.hasNext()) {
				int vertex = iter.next();
				if(sourceTypesToEdgesMap.containsKey(vertex) || objectTypesToEdgesMap.containsKey(vertex)) {
					// this is a type node
					// get all edges that have vertex as their source node
					if(sourceTypesToEdgesMap.containsKey(vertex)) {
						HashSet<Integer> srcEdges = sourceTypesToEdgesMap.get(vertex);
						Iterator<Integer> eiter = srcEdges.iterator();
						while(eiter.hasNext()) {
							int edge = eiter.next();
							if(edgeType.containsKey(edge)) {
								int othervertex = edgeType.get(edge).object_type;
								addTypeEdge(addedTuples, candidateGraph, candidateEdges, edge, vertex, othervertex);
							}
						}
					}
					// get all edges that have vertex as their object node
					if(objectTypesToEdgesMap.containsKey(vertex)) {
						HashSet<Integer> objEdges = objectTypesToEdgesMap.get(vertex);
						Iterator<Integer> eiter = objEdges.iterator();
						while(eiter.hasNext()) {
							int edge = eiter.next();
							if(edgeType.containsKey(edge)) {
								int othervertex = edgeType.get(edge).source_type;
								addTypeEdge(addedTuples, candidateGraph, candidateEdges, edge, othervertex, vertex);
							}
						}
					}
				} else {
					// if this vertex is an intermediate node, don't bother adding anything.
					if(isFreebaseDataset && intermediateNodesList.contains(vertex))
						continue;
					// this vertex is an actual node from the data graph.
					// add all those edges where vertex is the source...
					ArrayList<ObjNodeIntProperty> ons = null;
					if(loadDataGraph) {
						ons = srcDataGraph.get(vertex);
					} else {
						ons = sourceDataGraphFileHandler.getVertexNeighbors(vertex);
					}
					if(ons != null) {
						getCandidateEdge(candidateGraph, candidateEdges, seenTuples, addedTuples, vertex, ons);
					}

					// add all those edges where vertex is the object...
					ArrayList<ObjNodeIntProperty> revons = null;
					if(loadDataGraph) {
						revons = objDataGraph.get(vertex);
					} else {
						//System.out.println("finding candidate edges for entity = "+vertex);
						revons = objectDataGraphFileHandler.getVertexNeighbors(vertex);
					}
					if(revons != null) {
						getCandidateEdge(candidateGraph, candidateEdges, seenTuples, addedTuples, vertex, revons);
					}
				}
			}
		}
	}

	private void removeInitialEdgesFromTargetGraph(int src, int prop, int dest,
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph) {
		gqhelper.removeEdge(src, prop, dest, targetGraph);
		gqhelper.removeEdge(dest, prop, src, targetGraph);
	}

	private MutableString getTupleID(int src, int prop, int dest) {
		MutableString tid = new MutableString();
		if(src < dest) {
			tid = tid.append(src).append(",").append(prop).append(",").append(dest);
		}
		else {
			tid = tid.append(dest).append(",").append(prop).append(",").append(src);
		}
		return tid;
	}

	private void getCandidateEdge(HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph,
			HashSet<Integer> candidateEdges, HashSet<MutableString> seenTuples,
			HashSet<MutableString> addedTuples, int vertex, ArrayList<ObjNodeIntProperty> ons) {
		for(ObjNodeIntProperty on : ons) {
			if(isFreebaseDataset && intermediateNodesList.contains(on.dest)) {
				// get other nodes connected to this intermediate node.
				concatenateEdgesAndAddProp(candidateGraph, candidateEdges, seenTuples, addedTuples, vertex, on);
			}
			else {
				MutableString tid = getTupleID(vertex, on.prop, on.dest);
				if(!addedTuples.contains(tid) && !seenTuples.contains(tid)) {
					//	addedEdges.add(on.prop);
					EdgeEnds ee = new EdgeEnds();
					ee.source = vertex;
					ee.object = on.dest;
					addToCandidateGraph(candidateGraph, candidateEdges, addedTuples, on.prop, ee, tid);
				}
			}
		}
	}

	private void concatenateEdgesAndAddProp(HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph,
			HashSet<Integer> candidateEdges, HashSet<MutableString> seenTuples,
			HashSet<MutableString> addedTuples, int vertex, ObjNodeIntProperty on) {
		ArrayList<IntermediateNodeAndOtherEnd> intermediateEdges = getOtherEdgesOnIntermediateNode(vertex, on.prop, on.dest);
		for(IntermediateNodeAndOtherEnd interEdge : intermediateEdges) {
			String concatEdge = "";
			if(on.prop < interEdge.prop)
				concatEdge = on.prop + "," + interEdge.prop;
			else
				concatEdge = interEdge.prop + "," + on.prop;
			if(!concatenatedStringEdgesToNewEdgeIdMap.containsKey(concatEdge)) {
				// we have never seen this occurrence in our list. so ignore this.
				continue;
			}
			int newEdgeId = concatenatedStringEdgesToNewEdgeIdMap.get(concatEdge);
			MutableString tid = getTupleID(vertex, newEdgeId, interEdge.node);
			if(!addedTuples.contains(tid) && !seenTuples.contains(tid)) {
				EdgeEnds ee = new EdgeEnds();
				if(on.prop < interEdge.prop) {
					ee.source = vertex;
					ee.object = interEdge.node;
				}
				else {
					ee.source = interEdge.node;
					ee.object = vertex;
				}
				addToCandidateGraph(candidateGraph, candidateEdges, addedTuples, newEdgeId, ee, tid);
			}
		}
	}

	private ArrayList<IntermediateNodeAndOtherEnd> getOtherEdgesOnIntermediateNode(int firstNode, int firstProp, int intermediateNode) {
		ArrayList<IntermediateNodeAndOtherEnd> intermediateEdges = new ArrayList<IntermediateNodeAndOtherEnd>();
		ArrayList<ObjNodeIntProperty> intermediateNeighborsSrc = null;
		if(loadDataGraph) {
			intermediateNeighborsSrc = srcDataGraph.get(intermediateNode);
		} else {
			intermediateNeighborsSrc = sourceDataGraphFileHandler.getVertexNeighbors(intermediateNode);
		}

		if(intermediateNeighborsSrc != null && !intermediateNeighborsSrc.isEmpty()) {
			for(ObjNodeIntProperty internode : intermediateNeighborsSrc) {
				if(internode.prop != firstProp && internode.dest != firstNode) {
					IntermediateNodeAndOtherEnd ina = new IntermediateNodeAndOtherEnd();
					ina.node = internode.dest;
					ina.prop = internode.prop;
					intermediateEdges.add(ina);
				}
			}
		}

		ArrayList<ObjNodeIntProperty> intermediateNeighborsObj = null;
		if(loadDataGraph) {
			intermediateNeighborsObj = objDataGraph.get(intermediateNode);
		} else {
			intermediateNeighborsObj = objectDataGraphFileHandler.getVertexNeighbors(intermediateNode);
		}

		if(intermediateNeighborsObj != null && !intermediateNeighborsObj.isEmpty()) {
			for(ObjNodeIntProperty internode : intermediateNeighborsObj) {
				if(internode.prop != firstProp && internode.dest != firstNode) {
					IntermediateNodeAndOtherEnd ina = new IntermediateNodeAndOtherEnd();
					ina.node = internode.dest;
					ina.prop = internode.prop;
					intermediateEdges.add(ina);
				}
			}
		}
		return intermediateEdges;
	}

	private void addToCandidateGraph(HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph, HashSet<Integer> candidateEdges,
			HashSet<MutableString> addedTuples, int prop, EdgeEnds ee, MutableString tid) {
		candidateEdges.add(prop);
		addedTuples.add(tid);
		ArrayList<EdgeEnds> edges;
		if(candidateGraph.containsKey(prop)) {
			edges = candidateGraph.get(prop);
			edges.add(ee);
		}
		else {
			edges = new ArrayList<EdgeEnds>();
			edges.add(ee);
		}
		candidateGraph.put(prop, edges);
	}

	private ArrayList<EdgeEnds> isPositiveSuggestedEdge(String bestTriple, HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph,
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph) {
		// NOTE: the existence of the edge is a little loose here. A vertex can be connected to multiple instances of an edge label.
		// so if there is an edge say (a_1 -> e_1 -> a_2) in the target graph, and we have made the following suggestion (a_1 -> e_1 -> b_2)
		// then I would still say it is valid, and correct the suggested edge to the exact one in the target graph.
		// REASON to do this: If there are multiple instances of an edge associated with the same vertex, instead of making individual
		// suggestion to each of those instances, we can assume that given an edge associated with that vertex, we can expect the user
		// to identify the correct target node.
		ArrayList<EdgeEnds> positiveEdges = null;
		HashSet<MutableString> addedTuples = new HashSet<MutableString>();
		Iterator<Integer> iter = partialGraph.keySet().iterator();

		int pivotNode, otherNode, suggestedEdge;
		boolean isForwardEdge;
		if(!bestTriple.split(",")[0].equals("-1")) {
			pivotNode = Integer.parseInt(bestTriple.split(",")[0]);
			otherNode = Integer.parseInt(bestTriple.split(",")[2]);
			isForwardEdge = true;
		} else {
			pivotNode = Integer.parseInt(bestTriple.split(",")[2]);
			otherNode = Integer.parseInt(bestTriple.split(",")[0]);
			isForwardEdge = false;
		}
		suggestedEdge = Integer.parseInt(bestTriple.split(",")[1]);

		while(iter.hasNext()) {

			int v = iter.next();

			//if(model == ModelToUseForType.BLR) {
				//for baseline, make sure v is the same node as the pivotnode 
				//this unique way to check is implemented because partialGraph only has graphNodeId of dest node, not of dest node
				//therefore need to reverse check
				boolean matchFound = false;
				Iterator<Integer> propIter = partialGraph.get(v).keySet().iterator();
				int prop = propIter.next();
				int u = partialGraph.get(v).get(prop).get(0).getDest();
				for(DestNode dn : partialGraph.get(u).get(prop)) {
					if(dn.getDestGraphNodeID() == pivotNode) {
						matchFound = true;
					}
				}
				if(!matchFound) {
					continue;
				}
			//}
			
			if(targetGraph.containsKey(v)) {
				HashMap<Integer, ArrayList<DestNode>> propDest = targetGraph.get(v);
				if(propDest.containsKey(suggestedEdge)) {
					ArrayList<DestNode> dns = propDest.get(suggestedEdge);
					for(DestNode dn : dns) {
						if(!((isForwardEdge && dn.isForwardEdge()) || (!isForwardEdge && !dn.isForwardEdge()))) continue;
						int src, srcgn = -1;
						int dest, destgn = -1;
						if(dn.isForwardEdge()) {
							src = v;
							dest = dn.getDest();
							destgn = dn.getDestGraphNodeID();
							//if(model == ModelToUseForType.BLR && otherNode != -1 && destgn != otherNode) {
							if(otherNode != -1 && destgn != otherNode) {
								//for baseline, otherNode should match as well when suggesting edge between nodes
								continue;
							} 
							//get graphNode id for source node
							for(DestNode revdn : targetGraph.get(dest).get(suggestedEdge)) {
								if(!revdn.isForwardEdge() && revdn.getDest()==src) {
									srcgn = revdn.getDestGraphNodeID();
								} 
							}
						}
						else {
							src = dn.getDest();
							dest = v;
							srcgn = dn.getDestGraphNodeID();
							//if(model == ModelToUseForType.BLR && otherNode != -1 && srcgn != otherNode) {
							if(otherNode != -1 && srcgn != otherNode) {
								//for baseline, otherNode should match as well when suggesting edge between nodes
								continue;
							}
							//get graphNode id for dest node
							for(DestNode revdn : targetGraph.get(src).get(suggestedEdge)) {
								if(revdn.isForwardEdge() && revdn.getDest()==dest) {
									destgn = revdn.getDestGraphNodeID();
								} 
							}
						}
						MutableString tupleID = getTupleID(src, suggestedEdge, dest);
						if(!addedTuples.contains(tupleID)) {
							addedTuples.add(tupleID);
							if(positiveEdges == null) {
								positiveEdges = new ArrayList<EdgeEnds>();
							}
							EdgeEnds ee = new EdgeEnds();
							ee.source = src;
							ee.object = dest;
							ee.graphSource = srcgn;
							ee.graphObject = destgn;
							//ee.tupleID = tupleID;
							positiveEdges.add(ee);
						}
					}
				}
			}
		}
		return positiveEdges;
	}

	private void updateSeenTupleIDsListAndCandidateGraph(HashSet<MutableString> seenTuples, int prop,
			HashMap<Integer, ArrayList<EdgeEnds>> candidateGraph) {
		// add the newly added tuple IDs into seentuples list.
		// remove the entries associated with "prop" from candidateGraph.
		//System.out.println(prop);
		ArrayList<EdgeEnds> props = candidateGraph.get(prop);
		if(props != null) {
			for(EdgeEnds ee : props) {
				seenTuples.add(getTupleID(ee.source, prop, ee.object));
			}
			candidateGraph.remove(prop);
		}
	}

	private ArrayList<HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>>> makeDeepCopies(
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> targetGraph, int numOfCopies) {
		ArrayList<HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>>> allCopies =
				new ArrayList<HashMap<Integer,HashMap<Integer,ArrayList<DestNode>>>>();
		for(int i=0; i<numOfCopies; i++) {
			HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> tg = new HashMap<Integer, HashMap<Integer,ArrayList<DestNode>>>();
			Iterator<Integer> iter1 = targetGraph.keySet().iterator();
			while(iter1.hasNext()) {
				int vertex = iter1.next();
				HashMap<Integer, ArrayList<DestNode>> propDest = targetGraph.get(vertex);
				HashMap<Integer, ArrayList<DestNode>> newPropDest = new HashMap<Integer, ArrayList<DestNode>>();

				Iterator<Integer> iter2 = propDest.keySet().iterator();
				while(iter2.hasNext()) {
					int edge = iter2.next();
					ArrayList<DestNode> destnodes = propDest.get(edge);
					ArrayList<DestNode> newDestNodes = new ArrayList<DestNode>();

					for(DestNode dn : destnodes) {
						DestNode newDn = new DestNode();
						newDn.setDest(dn.getDest());
						newDn.setForwardEdge(dn.isForwardEdge());
						newDn.setDestGraphNodeID(dn.getDestGraphNodeID());
						newDestNodes.add(newDn);
					}
					newPropDest.put(edge, newDestNodes);
				}
				tg.put(vertex, newPropDest);
			}
			allCopies.add(tg);
		}
		return allCopies;
	}


	//converts simulated experiment graphs to gui graphs
	private ArrayList<GuiEdgeInfo> partialGraphToGuiGraph(HashMap<Integer, HashMap<Integer, ArrayList<DestNode>>> partialGraph) {
	
		ArrayList<GuiEdgeInfo> queryGraph = new ArrayList<GuiEdgeInfo>();

		LoadData ldlm = new LoadData();

		for(int src : partialGraph.keySet()) {
			for(int prop : partialGraph.get(src).keySet()) {
				for(DestNode dn : partialGraph.get(src).get(prop)) {
					int obj =  dn.getDest();
					if(!dn.isForwardEdge()) continue; //will not add reverse edge in the Gui graph
					GuiEdgeInfo gei = new GuiEdgeInfo();
					gei.edge = prop;
					gei.source = src;
					gei.object = obj;
					
					//get graphNode id for object node
					gei.graphObject = dn.getDestGraphNodeID();
					//get graphNode id for source node
					for(DestNode revdn : partialGraph.get(obj).get(prop)) {
						if(!revdn.isForwardEdge() && revdn.getDest()==src) {
							gei.graphSource = revdn.getDestGraphNodeID();
						} 
					}


					//add all types to the source and object nodes from user assigned type and incident edges
					if(!sourceTypesToEdgesMap.containsKey(src) && !objectTypesToEdgesMap.containsKey(src)) { 
						//source is an entity node
						gei.sourceEntity = Integer.toString(src);
						gei.sourceTypeValues = "";
					} else { 
						//source is a type node
						gei.sourceEntity = "-1";
						gei.sourceTypeValues = Integer.toString(src);
						for(int prop_ : partialGraph.get(src).keySet()) {
							for(DestNode dn_ : partialGraph.get(src).get(prop_)) {
								if(dn_.isForwardEdge()) {
									String t = Integer.toString(ldlm.getEdgeType().get(prop_).source_type);
									if(!gei.sourceTypeValues.contains(t)) gei.sourceTypeValues += ("," + t);
								} else {
									String t = Integer.toString(ldlm.getEdgeType().get(prop_).object_type);
									if(!gei.sourceTypeValues.contains(t)) gei.sourceTypeValues += ("," + t); 
								}
							}
						}
					}
					if(!sourceTypesToEdgesMap.containsKey(obj) && !objectTypesToEdgesMap.containsKey(obj)) {
						//object is an entity node
						gei.objectEntity = Integer.toString(obj);
						gei.objectTypeValues = "";
					} else {
						//object is a type node
						gei.objectEntity = "-1";
						gei.objectTypeValues = Integer.toString(obj);
						for(int prop_ : partialGraph.get(obj).keySet()) {
							for(DestNode dn_ : partialGraph.get(obj).get(prop_)) {
								if(dn_.isForwardEdge()) {
									String t = Integer.toString(ldlm.getEdgeType().get(prop_).source_type);
									if(!gei.objectTypeValues.contains(t)) gei.objectTypeValues += ("," + t);
								} else {
									String t = Integer.toString(ldlm.getEdgeType().get(prop_).object_type);
									if(!gei.objectTypeValues.contains(t)) gei.objectTypeValues += ("," + t); 
								}
							}
						}
					}

					queryGraph.add(gei);
				}
			}
		}
		// System.out.println("printing gui graph:");
		// for(GuiEdgeInfo gei: queryGraph) {
		// 	System.out.println(gei.graphSource+","+gei.source+","+gei.edge+","+gei.graphObject+","+gei.object+":"+gei.sourceTypeValues+":"+gei.objectTypeValues);
		// }
		return queryGraph;
	}

	private ArrayList<CandidateEdgeScore> rankCandidateEdges(HashSet<Integer> history, QueryResult qr, HashSet<Integer> candidateEdges, double logdf, String systemName) {
		//0 = DecisionForest;	1 = Random Forests;	2 = Naive Bayesian model;	3 = Random edge suggestion;
		//4 = recommendation systems (SVD);	5 = RandomSubsets;	6 = CAR
		//enum ModelToUseForType {DF, RF, NBC, RAND, SVD, RandSubsets, CAR};
		ArrayList<CandidateEdgeScore> edgeScores = new ArrayList<CandidateEdgeScore>();

		if(systemName.equals("orion")) {
			edgeScores = dfm.rankCandidateEdges(history, candidateEdges, logdf);
		} else if(systemName.equals("baseline")) {
			edgeScores = blr.rankCandidateEdges(candidateEdges, qr);
		}

		// case RF:
		// 	//bestEdge = rfp.findBestEdge(history, candidateEdges);
		// 	bestEdge = rf.findBestEdge(history, candidateEdges);
		// 	break;
		// case NBC:
		// 	//bestEdge = nbc.findBestEdge(history, candidateEdges);
		// 	bestEdge = nb.findBestEdge(history, candidateEdges);
		// 	break;
		// case RAND:
		// 	bestEdge = RandomEdgeSuggestor.getRandomEdge(candidateEdges);
		// 	break;
		// case SVD:
		// 	bestEdge = svd.findBestEdge(history, candidateEdges);
		// 	break;
		// case RandSubsets:
		// 	bestEdge = rsm.findBestEdge(history, candidateEdges);
		// 	break;
		// case CAR:
		// 	bestEdge = cba.findBestEdge(history, candidateEdges);
        //     break;
		// case ARM:
		// 	bestEdge = arm.findBestEdge(history, candidateEdges);
        //     break;
			// do things.
		return edgeScores;
	}

	private void initializeDBpediaDataStructures() {
		LoadData ldlm = new LoadData();
		if(loadDataGraph) {
			System.out.println("Loading data graph in memory!");
			ldlm.loadDataGraphIntPropertyDbpedia(conf.getInputFilePath(PropertyKeys.datagraphFile));
			srcDataGraph = LoadData.getSrcDataGraph();
			objDataGraph = LoadData.getObjDataGraph();
		}
		/**
		 * Using the same method as in freebase since the signature of the input file is the same, although the content is different
		 * The key in the hashmaps used here is actually an edge, not a type node as in Freebase's case.
		 */
		System.out.println("Loading type to edge list map, for source types");
		ldlm.loadSourceTypeToEdgesMap(conf.getInputFilePath(PropertyKeys.typeEdgesListSource));
		System.out.println("Loading type to edge list map, for object types");
		ldlm.loadObjectTypeToEdgesMap(conf.getInputFilePath(PropertyKeys.typeEdgesListObject));
		sourceEdgesToEdgesMapDbpedia = LoadData.getSourceTypesToEdgesMap();
		objectEdgesToEdgesMapDbpedia = LoadData.getObjectTypesToEdgesMap();
	}

	private void initializaFreebaseDataStructures() {
		LoadData ldlm = new LoadData();
		if(loadDataGraph) {
			// be sure to load data graph before loading edgeType and intermediate nodes list info.
			System.out.println("Loading data graph in memory!");
			ldlm.loadDataGraphIntProperty(conf.getInputFilePath(PropertyKeys.datagraphFile), loadDataGraph);
			srcDataGraph = LoadData.getSrcDataGraph();
			objDataGraph = LoadData.getObjDataGraph();
		}

		System.out.println("Loading type to edge list map, for source types");
		ldlm.loadSourceTypeToEdgesMap(conf.getInputFilePath(PropertyKeys.typeEdgesListSource));
		System.out.println("Loading type to edge list map, for object types");
		ldlm.loadObjectTypeToEdgesMap(conf.getInputFilePath(PropertyKeys.typeEdgesListObject));
		System.out.println("Loading edge type file");
		ldlm.loadEdgeTypeInfo(conf.getInputFilePath(PropertyKeys.edgeTypeFile));
		System.out.println("Populating intermediate nodes");
		ldlm.loadIntermediateNodesFromFile(conf.getInputFilePath(PropertyKeys.intermediateNodesFile));
		System.out.println("loading the concatenated property mapping list");
		ldlm.loadConcatedPropertiesList(conf.getOutputFilePath(PropertyKeys.barcelonaToFreebaseNewConcatenatedPropertiesFile));
		//for printing execution log 
		System.out.println("Load edge labels");
		ldlm.loadEdgeLabels(conf.getInputFilePath(PropertyKeys.propertiesLangEn));
		System.out.println("Load node labels");
		ldlm.loadAllNodeLabels(conf.getInputFilePath(PropertyKeys.domainLangEn), conf.getInputFilePath(PropertyKeys.typeLangEn), conf.getInputFilePath(PropertyKeys.entityLangEn));

		edgeType = LoadData.getEdgeType();
		intermediateNodesList = LoadData.getIntermediateNodesList();
		concatenatedStringEdgesToNewEdgeIdMap = LoadData.getConcatenatedStringEdgesToNewEdgeIdMap();
		sourceTypesToEdgesMap = LoadData.getSourceTypesToEdgesMap();
		objectTypesToEdgesMap = LoadData.getObjectTypesToEdgesMap();
	}

	private void learnModel() {
		//0 = DecisionForest;	1 = Random Forests;	2 = Naive Bayesian model;	3 = Random edge suggestion;
		//4 = recommendation systems (SVD);	5 = RandomSubsets;	6 = CAR
		//enum ModelToUseForType {DF, RF, NBC, RAND, SVD, RandSubsts, CAR};
		switch(model) {
		case DF:
			dfm = new DecisionForestMain(conf);
			System.out.println("Learning model DECISON FOREST");
			//			logger.info("Learning model DECISON FOREST");
			dfm.learnModel(numOfRandomSubsets, isWeightedConf, topkRules, countCondition, historyUpdate);

			// //also learning baseline, in case we want to switch/combine
			// blr = new BaseLineRanker(conf);
			// System.out.println("Learning model BLR");
			// blr.learnModel();
			break;
		case RF:
			//rfp = new RandomForestPredict();
			rf = new RandomForest();
			System.out.println("Learning model Random FOREST");
			//			logger.info("Learning model Random FOREST");
			//rfp.learnModel();
			break;
		case NBC:
			//nbc = new NaiveBayesianMain(conf);
			nb = new NaiveBayes();
			System.out.println("Learning model Naive BAYESIAN");
			//			logger.info("Learning model Naive BAYESIAN");
			//nbc.learnModel();
			break;
		case RAND:
			res = new RandomEdgeSuggestor(conf);
			System.out.println("Learning model RANDOM crazy");
			//			logger.info("Learning model RANDOM crazy");
			res.learnModel();
			break;
			// do things.
		case SVD:
			svd = new RecommendationSystem();
			System.out.println("Learning model SVD based recommendation system");
			//			logger.info("Learning model SVD based recommendation system");
			svd.learnModel();
			break;
		case RandSubsets:
			rsm = new RandomSubsetsMain(conf);
			System.out.println("Learning model random subsets");
			//			logger.info("Learning model random subsets");
			rsm.learnModel();
			break;
		case CAR:
            cba = new Cba();
			System.out.println("Learning model CAR");
			//			logger.info("Learning model SVM");
            //cba.learnModel();
			break;
		case ARM:
            arm = new AssociationRuleRecommender();
			System.out.println("Learning model ARM");
			//			logger.info("Learning model SVM");
            arm.learnModel();
			break;
		case BLR:
            blr = new BaseLineRanker(conf);
			System.out.println("Learning model BLR");
			blr.learnModel();
			//			logger.info("Learning model SVM");
			break;
		}
	}
}